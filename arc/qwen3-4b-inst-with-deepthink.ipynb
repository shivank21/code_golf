{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe2e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:54:46.386350Z",
     "iopub.status.busy": "2025-09-01T06:54:46.385799Z",
     "iopub.status.idle": "2025-09-01T06:54:46.594772Z",
     "shell.execute_reply": "2025-09-01T06:54:46.594157Z"
    },
    "papermill": {
     "duration": 0.213291,
     "end_time": "2025-09-01T06:54:46.595941",
     "exception": false,
     "start_time": "2025-09-01T06:54:46.382650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!du -sh /kaggle/input/notebook-qwen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cc369",
   "metadata": {},
   "source": [
    "## Installing Dependencies through a Utility Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d9f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:54:46.602490Z",
     "iopub.status.busy": "2025-09-01T06:54:46.601918Z",
     "iopub.status.idle": "2025-09-01T06:57:28.180768Z",
     "shell.execute_reply": "2025-09-01T06:57:28.180070Z"
    },
    "papermill": {
     "duration": 161.583425,
     "end_time": "2025-09-01T06:57:28.182099",
     "exception": false,
     "start_time": "2025-09-01T06:54:46.598674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-index --find-links=/kaggle/input/notebook-qwen -U torch==2.6.0 torchaudio==2.6.0 torchvision==0.21.0 trl==0.17.0 bitsandbytes==0.45.5 pyzmq==26.4 vllm==0.8.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6a7eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:57:28.200384Z",
     "iopub.status.busy": "2025-09-01T06:57:28.200150Z",
     "iopub.status.idle": "2025-09-01T06:57:28.207512Z",
     "shell.execute_reply": "2025-09-01T06:57:28.206979Z"
    },
    "papermill": {
     "duration": 0.017351,
     "end_time": "2025-09-01T06:57:28.208469",
     "exception": false,
     "start_time": "2025-09-01T06:57:28.191118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/input/vllm-deepth/vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48778aa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:57:28.225835Z",
     "iopub.status.busy": "2025-09-01T06:57:28.225642Z",
     "iopub.status.idle": "2025-09-01T06:57:28.228214Z",
     "shell.execute_reply": "2025-09-01T06:57:28.227749Z"
    },
    "papermill": {
     "duration": 0.012182,
     "end_time": "2025-09-01T06:57:28.229085",
     "exception": false,
     "start_time": "2025-09-01T06:57:28.216903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !VLLM_USE_PRECOMPILED=1 pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33670b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:57:28.246501Z",
     "iopub.status.busy": "2025-09-01T06:57:28.246130Z",
     "iopub.status.idle": "2025-09-01T06:57:28.249640Z",
     "shell.execute_reply": "2025-09-01T06:57:28.249153Z"
    },
    "papermill": {
     "duration": 0.013278,
     "end_time": "2025-09-01T06:57:28.250709",
     "exception": false,
     "start_time": "2025-09-01T06:57:28.237431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090d28f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:57:28.268214Z",
     "iopub.status.busy": "2025-09-01T06:57:28.267770Z",
     "iopub.status.idle": "2025-09-01T06:57:32.508658Z",
     "shell.execute_reply": "2025-09-01T06:57:32.507977Z"
    },
    "papermill": {
     "duration": 4.250848,
     "end_time": "2025-09-01T06:57:32.509972",
     "exception": false,
     "start_time": "2025-09-01T06:57:28.259124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show transformers trl bitsandbytes torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f2e73",
   "metadata": {},
   "source": [
    "## Running the Actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a3e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T06:57:32.528542Z",
     "iopub.status.busy": "2025-09-01T06:57:32.528325Z",
     "iopub.status.idle": "2025-09-01T07:08:52.528461Z",
     "shell.execute_reply": "2025-09-01T07:08:52.527750Z"
    },
    "papermill": {
     "duration": 680.010777,
     "end_time": "2025-09-01T07:08:52.529863",
     "exception": false,
     "start_time": "2025-09-01T06:57:32.519086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /kaggle/input/qwen-confidence-try/qwen_confdemo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbf5f043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T07:08:52.623510Z",
     "iopub.status.busy": "2025-09-01T07:08:52.623220Z",
     "iopub.status.idle": "2025-09-01T07:08:52.626473Z",
     "shell.execute_reply": "2025-09-01T07:08:52.625934Z"
    },
    "papermill": {
     "duration": 0.073645,
     "end_time": "2025-09-01T07:08:52.627339",
     "exception": false,
     "start_time": "2025-09-01T07:08:52.553694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vllm==0.8.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6113bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T07:08:52.670568Z",
     "iopub.status.busy": "2025-09-01T07:08:52.670185Z",
     "iopub.status.idle": "2025-09-01T07:08:52.672817Z",
     "shell.execute_reply": "2025-09-01T07:08:52.672317Z"
    },
    "papermill": {
     "duration": 0.025023,
     "end_time": "2025-09-01T07:08:52.673715",
     "exception": false,
     "start_time": "2025-09-01T07:08:52.648692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python /kaggle/input/qwen-kuch-4b/qwen_base.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a5ac3",
   "metadata": {},
   "source": [
    "## Visialise Results(Same as sample nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aa442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-01T07:08:52.716531Z",
     "iopub.status.busy": "2025-09-01T07:08:52.716354Z",
     "iopub.status.idle": "2025-09-01T07:08:56.022592Z",
     "shell.execute_reply": "2025-09-01T07:08:56.021956Z"
    },
    "papermill": {
     "duration": 3.329031,
     "end_time": "2025-09-01T07:08:56.023672",
     "exception": false,
     "start_time": "2025-09-01T07:08:52.694641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add visualization functions to the notebook\n",
    "import os\n",
    "import json\n",
    "fake_mode = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "if fake_mode:\n",
    "    arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json'\n",
    "else:\n",
    "    arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(arc_challenge_file, 'r') as f:\n",
    "    arc_data = json.load(f)\n",
    "\n",
    "training_solution_path = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json'\n",
    "with open(training_solution_path, 'r') as f:\n",
    "    training_solution = json.load(f)\n",
    "evaluation_solution_path = '/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json'\n",
    "with open(evaluation_solution_path, 'r') as f:\n",
    "    evaluation_solution = json.load(f)\n",
    "        \n",
    "def visualize_arc_results():\n",
    "    \"\"\"Visualize ARC problem solutions from submission.json\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import colors\n",
    "    import json\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VISUALIZING ARC SOLUTION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if submission file exists\n",
    "    submission_path = 'submission.json'\n",
    "    if not os.path.exists(submission_path):\n",
    "        print(f\"Submission file not found at {submission_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found submission file: {submission_path}\")\n",
    "    \n",
    "    # Load submission data\n",
    "    with open(submission_path, 'r') as f:\n",
    "        submission_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded submission with {len(submission_data)} tasks\")\n",
    "    \n",
    "    # ARC color map - colors for values 0-9\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    \n",
    "    # Function to check if prediction is non-trivial (not just zeros)\n",
    "    def is_non_trivial_prediction(pred_array):\n",
    "        # Check if the prediction contains any non-zero values\n",
    "        return np.any(np.array(pred_array) > 0)\n",
    "    \n",
    "    # Function to visualize a single task result\n",
    "    def visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "        # Skip visualization if both predictions are just zeros\n",
    "        pred_1 = np.array(submission_output['attempt_1'])\n",
    "        pred_2 = np.array(submission_output['attempt_2'])\n",
    "        \n",
    "        if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "            print(f\"  Skipping visualization for Task {task_id} - Test #{test_idx+1} (all predictions are zeros)\")\n",
    "            return False\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        grid_spec = plt.GridSpec(2, 3, width_ratios=[1, 1, 1])\n",
    "        \n",
    "        # Training examples (first one only for simplicity)\n",
    "        if task_data['train']:\n",
    "            # Train Input\n",
    "            ax1 = fig.add_subplot(grid_spec[0, 0])\n",
    "            ax1.imshow(task_data['train'][0]['input'], cmap=cmap, norm=norm)\n",
    "            ax1.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax1.set_title(\"Training Input\")\n",
    "            ax1.set_xticks([])\n",
    "            ax1.set_yticks([])\n",
    "            \n",
    "            # Train Output\n",
    "            ax2 = fig.add_subplot(grid_spec[1, 0])\n",
    "            ax2.imshow(task_data['train'][0]['output'], cmap=cmap, norm=norm)\n",
    "            ax2.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax2.set_title(\"Training Output\")\n",
    "            ax2.set_xticks([])\n",
    "            ax2.set_yticks([])\n",
    "        \n",
    "        # Test Input\n",
    "        if test_idx < len(task_data['test']):\n",
    "            ax3 = fig.add_subplot(grid_spec[0, 1])\n",
    "            ax3.imshow(task_data['test'][test_idx]['input'], cmap=cmap, norm=norm)\n",
    "            ax3.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "            ax3.set_title(f\"Test Input (Test #{test_idx+1})\")\n",
    "            ax3.set_xticks([])\n",
    "            ax3.set_yticks([])\n",
    "        \n",
    "        # Model Predictions\n",
    "        # Attempt 1\n",
    "        ax5 = fig.add_subplot(grid_spec[0, 2])\n",
    "        ax5.imshow(pred_1, cmap=cmap, norm=norm)\n",
    "        ax5.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        ax5.set_title(\"Model Prediction (Attempt 1)\")\n",
    "        ax5.set_xticks([])\n",
    "        ax5.set_yticks([])\n",
    "        \n",
    "        # Attempt 2\n",
    "        ax6 = fig.add_subplot(grid_spec[1, 2])\n",
    "        ax6.imshow(pred_2, cmap=cmap, norm=norm)\n",
    "        ax6.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "        ax6.set_title(\"Model Prediction (Attempt 2)\")\n",
    "        ax6.set_xticks([])\n",
    "        ax6.set_yticks([])\n",
    "        \n",
    "        # If ground truth is available and we're in fake/debug mode\n",
    "        if (task_id in arc_data) and (task_id in training_solution or task_id in evaluation_solution):\n",
    "            # Get ground truth\n",
    "            ground_truth = None\n",
    "            if task_id in training_solution and len(training_solution[task_id]) > test_idx:\n",
    "                ground_truth = training_solution[task_id][test_idx]\n",
    "            elif task_id in evaluation_solution and len(evaluation_solution[task_id]) > test_idx:\n",
    "                ground_truth = evaluation_solution[task_id][test_idx]\n",
    "                \n",
    "            if ground_truth:\n",
    "                ax4 = fig.add_subplot(grid_spec[1, 1])\n",
    "                ax4.imshow(ground_truth, cmap=cmap, norm=norm)\n",
    "                ax4.grid(True, which='both', color='lightgrey', linewidth=0.5)\n",
    "                ax4.set_title(\"Ground Truth\")\n",
    "                ax4.set_xticks([])\n",
    "                ax4.set_yticks([])\n",
    "                \n",
    "                # Calculate match information\n",
    "                match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                \n",
    "                # Add match indicators to prediction titles\n",
    "                ax5.set_title(f\"Prediction 1: {'✓' if match_1 else '✗'}\")\n",
    "                ax6.set_title(f\"Prediction 2: {'✓' if match_2 else '✗'}\")\n",
    "                \n",
    "                # Display match information\n",
    "                print(f\"  Results: Attempt 1: {'✓' if match_1 else '✗'}, Attempt 2: {'✓' if match_2 else '✗'}\")\n",
    "                print(f\"  Shape - Ground Truth: {np.array(ground_truth).shape}, \"\n",
    "                      f\"Prediction 1: {pred_1.shape}, Prediction 2: {pred_2.shape}\")\n",
    "                print(f\"  Values - Ground Truth unique values: {np.unique(ground_truth)}\")\n",
    "                print(f\"          Prediction 1 unique values: {np.unique(pred_1)}\")\n",
    "                print(f\"          Prediction 2 unique values: {np.unique(pred_2)}\")\n",
    "        \n",
    "        plt.suptitle(f\"Task {task_id} - Test Example #{test_idx+1}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "        return True\n",
    "    \n",
    "    # Process all results from submission\n",
    "    visualized_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Create a list of all tasks and their test indices\n",
    "    all_predictions = []\n",
    "    for task_id in submission_data:\n",
    "        if task_id in arc_data:\n",
    "            task_data = arc_data[task_id]\n",
    "            for test_idx, test_prediction in enumerate(submission_data[task_id]):\n",
    "                # Check if predictions are non-trivial\n",
    "                pred_1 = np.array(test_prediction['attempt_1'])\n",
    "                pred_2 = np.array(test_prediction['attempt_2'])\n",
    "                has_non_zero_pred = is_non_trivial_prediction(pred_1) or is_non_trivial_prediction(pred_2)\n",
    "                \n",
    "                # Check if we have ground truth available\n",
    "                has_ground_truth = False\n",
    "                correct_count = 0\n",
    "                \n",
    "                if task_id in training_solution and len(training_solution[task_id]) > test_idx:\n",
    "                    has_ground_truth = True\n",
    "                    ground_truth = training_solution[task_id][test_idx]\n",
    "                    \n",
    "                    if has_non_zero_pred:\n",
    "                        match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                        match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                        correct_count = int(match_1) + int(match_2)\n",
    "                \n",
    "                elif task_id in evaluation_solution and len(evaluation_solution[task_id]) > test_idx:\n",
    "                    has_ground_truth = True\n",
    "                    ground_truth = evaluation_solution[task_id][test_idx]\n",
    "                    \n",
    "                    if has_non_zero_pred:\n",
    "                        match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                        match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                        correct_count = int(match_1) + int(match_2)\n",
    "                \n",
    "                all_predictions.append((task_id, test_idx, correct_count, has_ground_truth, has_non_zero_pred))\n",
    "    \n",
    "    # Sort predictions by correctness and ground truth availability\n",
    "    all_predictions.sort(key=lambda x: (-int(x[3]), -x[2]))\n",
    "    \n",
    "    print(f\"\\nFound {len(all_predictions)} total predictions to visualize\")\n",
    "    \n",
    "    # Limit visualization to first N samples for performance\n",
    "    max_samples = 10  # Change this number to see more or fewer examples\n",
    "    samples_to_show = all_predictions[:max_samples]\n",
    "    \n",
    "    print(f\"Showing {len(samples_to_show)} of {len(all_predictions)} prediction samples\")\n",
    "    \n",
    "    # Visualize selected predictions\n",
    "    for task_id, test_idx, correct_count, has_ground_truth, has_non_zero_pred in samples_to_show:\n",
    "        task_data = arc_data[task_id]\n",
    "        submission_output = submission_data[task_id][test_idx]\n",
    "        \n",
    "        # Visualize this task\n",
    "        score_info = f\" (Score: {correct_count}/2)\" if has_ground_truth and has_non_zero_pred else \" (no ground truth)\" if not has_ground_truth else \" (all zeros - no score)\"\n",
    "        print(f\"\\nTask: {task_id} - Test #{test_idx+1}{score_info}\")\n",
    "        \n",
    "        # Only increment visualized_count if actually visualized\n",
    "        if visualize_submission_result(task_id, task_data, submission_output, test_idx):\n",
    "            visualized_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"\\nVisualized {visualized_count} inference results (skipped {skipped_count} with all-zero predictions)\")\n",
    "    \n",
    "    # Calculate overall accuracy statistics if in fake/debug mode\n",
    "    if fake_mode:  # Only run statistics in debug/local mode\n",
    "        total_tests = 0\n",
    "        total_scored_tests = 0\n",
    "        correct_attempt1 = 0\n",
    "        correct_attempt2 = 0\n",
    "        correct_any = 0\n",
    "        zero_predictions = 0\n",
    "        \n",
    "        for task_id, test_idx, _, has_ground_truth, _ in all_predictions:\n",
    "            if has_ground_truth:\n",
    "                total_tests += 1\n",
    "                \n",
    "                # Get ground truth\n",
    "                ground_truth = None\n",
    "                if task_id in training_solution and len(training_solution[task_id]) > test_idx:\n",
    "                    ground_truth = training_solution[task_id][test_idx]\n",
    "                elif task_id in evaluation_solution and len(evaluation_solution[task_id]) > test_idx:\n",
    "                    ground_truth = evaluation_solution[task_id][test_idx]\n",
    "                \n",
    "                if not ground_truth:\n",
    "                    continue\n",
    "                    \n",
    "                pred_1 = np.array(submission_data[task_id][test_idx]['attempt_1'])\n",
    "                pred_2 = np.array(submission_data[task_id][test_idx]['attempt_2'])\n",
    "                \n",
    "                # Check if both predictions are all zeros\n",
    "                if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n",
    "                    zero_predictions += 1\n",
    "                    continue\n",
    "                \n",
    "                # Only count tests with at least one non-zero prediction\n",
    "                total_scored_tests += 1\n",
    "                \n",
    "                match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n",
    "                match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n",
    "                \n",
    "                if match_1: correct_attempt1 += 1\n",
    "                if match_2: correct_attempt2 += 1\n",
    "                if match_1 or match_2: correct_any += 1\n",
    "        \n",
    "        if total_tests > 0:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"OVERALL ACCURACY STATISTICS\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Total test examples: {total_tests}\")\n",
    "            print(f\"Test examples with zero predictions (excluded from accuracy): {zero_predictions}\")\n",
    "            print(f\"Test examples included in accuracy calculation: {total_scored_tests}\")\n",
    "            \n",
    "            if total_scored_tests > 0:\n",
    "                print(f\"Correct on attempt 1: {correct_attempt1}/{total_scored_tests} ({correct_attempt1/total_scored_tests:.2%})\")\n",
    "                print(f\"Correct on attempt 2: {correct_attempt2}/{total_scored_tests} ({correct_attempt2/total_scored_tests:.2%})\")\n",
    "                print(f\"Correct on either attempt: {correct_any}/{total_scored_tests} ({correct_any/total_scored_tests:.2%})\")\n",
    "            else:\n",
    "                print(\"No non-zero predictions to calculate accuracy\")\n",
    "                \n",
    "            print(f\"Overall completion rate: {total_scored_tests/total_tests:.2%} of tests have non-zero predictions\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "# Add this line to the notebook to call the visualization function\n",
    "# Call after your submission.json has been created\n",
    "if fake_mode:\n",
    "    visualize_arc_results()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8160276,
     "sourceId": 12897189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8160712,
     "sourceId": 12897848,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8163739,
     "sourceId": 12902566,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8164704,
     "sourceId": 12904109,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 258708119,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 258850973,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301506,
     "sourceId": 363124,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301514,
     "sourceId": 363134,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301532,
     "sourceId": 363156,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 854.489388,
   "end_time": "2025-09-01T07:08:56.425015",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-01T06:54:41.935627",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
